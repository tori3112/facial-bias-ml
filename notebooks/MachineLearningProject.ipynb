{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tori3112/facial-bias-ml/blob/main/MachineLearningProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U34SOSlyCv35"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVdDtRxDCwn-"
      },
      "source": [
        "# Investigation into Ethnic Bias in Face Recognition Tasks\n",
        "\n",
        "1. Load the dataset.\n",
        "2. Prepare images.\n",
        "3. Make up 4 datasets with varying ethinic labels ratios.\n",
        "4. Build the CNN model.\n",
        "5. Evaluate the model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating a Pipeline**"
      ],
      "metadata": {
        "id": "dkCFtJy65o1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 0 : importing"
      ],
      "metadata": {
        "id": "5jjJz7SyB4UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_imports():\n",
        "   \"\"\"Import all required libraries for the pipeline\"\"\"\n",
        "   global os, drive, np, Image, ET, train_test_split\n",
        "   global VGG16, Model, Dense, GlobalAveragePooling2D, Dropout\n",
        "   global Adam, EarlyStopping, ReduceLROnPlateau\n",
        "   global plt, classification_report, accuracy_score\n",
        "\n",
        "   import os\n",
        "   import json\n",
        "   import numpy as np\n",
        "   from PIL import Image\n",
        "   import xml.etree.ElementTree as ET\n",
        "   from sklearn.model_selection import train_test_split\n",
        "   from tensorflow.keras.applications import VGG16\n",
        "   from tensorflow.keras.models import Model\n",
        "   from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "   from tensorflow.keras.optimizers import Adam\n",
        "   from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "   from google.colab import drive\n",
        "   import matplotlib.pyplot as plt\n",
        "   from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "\n",
        "   print(\"All libraries imported successfully\")"
      ],
      "metadata": {
        "id": "fDuytOajCOGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 1 : Creating Suitable Training Data from dataset in filepath"
      ],
      "metadata": {
        "id": "nWIrGGb0-lW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_training_data(training_filepath):\n",
        "    \"\"\"\n",
        "    First function in the pipeline that processes the training data\n",
        "    Input: training_filepath - path to zip file in Google Drive\n",
        "    Output: X (images), y (labels), id_to_label mapping\n",
        "    \"\"\"\n",
        "    # Mount drive if not already mounted\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    # Get the base name of the zip file (without .zip extension)\n",
        "    base_name = os.path.splitext(os.path.basename(training_filepath))[0]\n",
        "\n",
        "    # Create the dataset path based on the zip file name\n",
        "    dataset_path = f'/content/dataset/{base_name}'\n",
        "\n",
        "    # Ensure dataset directory exists and unzip\n",
        "    if not os.path.exists('/content/dataset'):\n",
        "        os.makedirs('/content/dataset')\n",
        "\n",
        "    # Only unzip if the dataset directory is empty\n",
        "    if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n",
        "        from zipfile import ZipFile\n",
        "        print(f\"Unzipping {training_filepath} to {dataset_path}\")\n",
        "        try:\n",
        "            with ZipFile(training_filepath, 'r') as zip_ref:\n",
        "                zip_ref.extractall('/content/dataset')\n",
        "            print(\"Unzipping completed successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during unzipping: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    print(\"Dataset contents:\", os.listdir(dataset_path))\n",
        "\n",
        "    #def parse_xml_labels(xml_path, dataset_path):\n",
        "        #tree = ET.parse(xml_path)\n",
        "        #root = tree.getroot()\n",
        "\n",
        "        # Get the list of folders in the dataset directory\n",
        "        #existing_folders = set(os.listdir(dataset_path))\n",
        "\n",
        "        #id_to_label = {}\n",
        "        #for subject in root.findall('subject'):\n",
        "            #person_id = subject.find('id').text\n",
        "            #if person_id in existing_folders:\n",
        "                #id_to_label[person_id] = len(id_to_label)\n",
        "\n",
        "        #return id_to_label\n",
        "\n",
        "    def parse_xml_labels(xml_path, dataset_path):\n",
        "      tree = ET.parse(xml_path)\n",
        "      root = tree.getroot()\n",
        "\n",
        "      # Get all person IDs from all ethnicity folders\n",
        "      all_person_folders = set()\n",
        "      for ethnicity_folder in os.listdir(dataset_path):\n",
        "          ethnicity_path = os.path.join(dataset_path, ethnicity_folder)\n",
        "          all_person_folders.update(os.listdir(ethnicity_path))\n",
        "\n",
        "      # Create the label mapping\n",
        "      id_to_label = {}\n",
        "      for subject in root.findall('subject'):\n",
        "          person_id = subject.find('id').text\n",
        "          if person_id in all_person_folders:\n",
        "              id_to_label[person_id] = len(id_to_label)\n",
        "\n",
        "      return id_to_label\n",
        "\n",
        "    def load_dataset(dataset_path, id_to_label):\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        # First level: ethnicity folders\n",
        "        for ethnicity_folder in os.listdir(dataset_path):\n",
        "            ethnicity_path = os.path.join(dataset_path, ethnicity_folder)\n",
        "            if not os.path.isdir(ethnicity_path):\n",
        "                continue\n",
        "\n",
        "            print(f\"Processing ethnicity folder: {ethnicity_folder}\")\n",
        "\n",
        "            # Second level: person folders within each ethnicity folder\n",
        "            for person_folder in os.listdir(ethnicity_path):\n",
        "                person_path = os.path.join(ethnicity_path, person_folder)\n",
        "                if not os.path.isdir(person_path):\n",
        "                    continue\n",
        "\n",
        "                if person_folder in id_to_label:\n",
        "                    label = id_to_label[person_folder]\n",
        "\n",
        "                    # Third level: image files within person folders\n",
        "                    for img_name in os.listdir(person_path):\n",
        "                        if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                            img_path = os.path.join(person_path, img_name)\n",
        "                            try:\n",
        "                                img = Image.open(img_path)\n",
        "                                img = img.resize((224, 224))  # Resize for VGG16\n",
        "                                img_array = np.array(img, dtype=np.float32)\n",
        "\n",
        "                                # Handle grayscale images\n",
        "                                if len(img_array.shape) == 2:\n",
        "                                    img_array = np.stack((img_array,)*3, axis=-1)\n",
        "\n",
        "                                # Normalize pixel values\n",
        "                                img_array /= 255.0\n",
        "\n",
        "                                images.append(img_array)\n",
        "                                labels.append(label)\n",
        "                            except Exception as e:\n",
        "                                print(f\"Error loading {img_path}: {e}\")\n",
        "\n",
        "        return np.array(images), np.array(labels)\n",
        "\n",
        "    # Note: XML file stays in Google Drive but dataset is in local Colab storage\n",
        "    xml_path = '/content/drive/MyDrive/finalTrain.xml'\n",
        "\n",
        "    # Create label mapping\n",
        "    id_to_label = parse_xml_labels(xml_path, dataset_path)\n",
        "    print(f\"Found {len(id_to_label)} unique individuals\")\n",
        "\n",
        "    # Load the dataset\n",
        "    X, y = load_dataset(dataset_path, id_to_label)\n",
        "\n",
        "    print(f\"Processing dataset from: {base_name}\")\n",
        "    print(f\"Final dataset shapes - X: {X.shape}, y: {y.shape}\")\n",
        "\n",
        "    return X, y, id_to_label, base_name"
      ],
      "metadata": {
        "id": "5RElDseqgYsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 2 : Defining Testing Data"
      ],
      "metadata": {
        "id": "l4cwYJIpMbpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(X, y):\n",
        "   \"\"\"\n",
        "   Second function in the pipeline that splits the data\n",
        "   \"\"\"\n",
        "   # First split off test set (20%)\n",
        "   X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "       X, y,\n",
        "       test_size=0.2,\n",
        "       stratify=y,\n",
        "       random_state=42\n",
        "   )\n",
        "\n",
        "   # Split remaining data into train (60%) and validation (20%)\n",
        "   X_train, X_val, y_train, y_val = train_test_split(\n",
        "       X_temp, y_temp,\n",
        "       test_size=0.25,\n",
        "       stratify=y_temp,\n",
        "       random_state=42\n",
        "   )\n",
        "\n",
        "   print(\"\\nSplit sizes:\")\n",
        "   print(f\"Training: {X_train.shape[0]} images\")\n",
        "   print(f\"Validation: {X_val.shape[0]} images\")\n",
        "   print(f\"Test: {X_test.shape[0]} images\")\n",
        "\n",
        "   return X_train, X_val, X_test, y_train, y_val, y_test\n"
      ],
      "metadata": {
        "id": "YPQi6VbvWEvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 3 : Creating the Model"
      ],
      "metadata": {
        "id": "mjuJzhWmZQF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WNlbNDs5aPsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_classes):\n",
        "    \"\"\"\n",
        "    Third function in the pipeline that creates the model architecture\n",
        "    Input: number of classes (from len(id_to_label))\n",
        "    Output: compiled model\n",
        "    \"\"\"\n",
        "    # 2. Load the base VGG16 model\n",
        "    base_model = VGG16(\n",
        "        weights='imagenet',      # Use pre-trained weights from ImageNet\n",
        "        include_top=False,       # Don't include the original classification layers\n",
        "        input_shape=(224, 224, 3) # Our image dimensions\n",
        "    )\n",
        "\n",
        "    # 3. Freeze the VGG16 layers (we don't want to train them initially)\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # 4. Add our own custom classification layers on top of VGG16\n",
        "    x = base_model.output\n",
        "    # Add Global Average Pooling to reduce dimensions\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    # Add first Dense layer with dropout\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)  # Prevent overfitting\n",
        "    # Add second Dense layer with dropout\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    # Final classification layer\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # 5. Create the full model\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # 6. Compile the model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0001),  # Conservative learning rate\n",
        "        loss='sparse_categorical_crossentropy', # Because our labels are integers\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "IQf8fX-wZVgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 4 : Train the model"
      ],
      "metadata": {
        "id": "AueIN8FRaQqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_model_exists(base_name):\n",
        "    model_path = f'/content/drive/MyDrive/savedModels/face_recognition_VGG16_{base_name}.h5'\n",
        "    return os.path.exists(model_path)"
      ],
      "metadata": {
        "id": "PoMXzj92ftJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, y_train, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Fourth function in the pipeline that trains the model\n",
        "    Input: model and training/validation data\n",
        "    Output: trained model and training history\n",
        "    \"\"\"\n",
        "    # 7. Set up callbacks for better training\n",
        "    callbacks = [\n",
        "        # Early stopping to prevent overfitting\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,         # Wait 10 epochs before stopping\n",
        "            restore_best_weights=True  # Keep the best weights\n",
        "        ),\n",
        "        # Reduce learning rate when improvement slows\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.2,         # Reduce LR by 80% when plateau occurs\n",
        "            patience=5,         # Wait 5 epochs before reducing\n",
        "            min_lr=1e-6        # Don't go below this LR\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # 8. Train the model\n",
        "    print(\"Starting training...\")\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=50,             # Maximum number of epochs\n",
        "        batch_size=32,         # Batch size - adjust based on your GPU memory\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    print(f\"Training with {X_train.shape[0]} images across {len(np.unique(y_train))} classes\")\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "3Mq1dFeuaWeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Model Accuracy per Ethnicity"
      ],
      "metadata": {
        "id": "ooAdlmITbd5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_saved_model(base_name):\n",
        "    model_path = f'/content/drive/MyDrive/savedModels/face_recognition_VGG16_{base_name}.h5'\n",
        "    return tf.keras.models.load_model(model_path)"
      ],
      "metadata": {
        "id": "c1pV5oVPoYcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test, id_to_label, base_name):\n",
        "    \"\"\"\n",
        "    Fifth function in the pipeline that evaluates model accuracy per ethnicity\n",
        "    Input: model, test data, label mapping\n",
        "    Output: accuracies per ethnicity group\n",
        "    \"\"\"\n",
        "    # Get predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    # Load XML file to get ethnicity information\n",
        "    tree = ET.parse('/content/drive/MyDrive/finalTrain.xml')\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Create mapping of person ID to ethnicity\n",
        "    id_to_ethnicity = {}\n",
        "    for subject in root.findall('subject'):\n",
        "        person_id = subject.find('id').text\n",
        "        ethnicity = int(subject.find('ethnicity').text)\n",
        "        id_to_ethnicity[person_id] = ethnicity\n",
        "\n",
        "    # Create reverse mapping for label to ID\n",
        "    label_to_id = {v: k for k, v in id_to_label.items()}\n",
        "\n",
        "    # Calculate accuracy per ethnicity\n",
        "    ethnicity_mapping = {\n",
        "        1: 'African American',\n",
        "        2: 'Asian Indian',\n",
        "        3: 'Caucasian Latin',\n",
        "        4: 'East Asian'\n",
        "    }\n",
        "\n",
        "    # Store accuracies\n",
        "    ethnicity_accuracies = {}\n",
        "\n",
        "    for eth_num, eth_name in ethnicity_mapping.items():\n",
        "        # Get indices of test samples for this ethnicity\n",
        "        eth_indices = []\n",
        "        for i, label in enumerate(y_test):\n",
        "            person_id = label_to_id[label]\n",
        "            if id_to_ethnicity[person_id] == eth_num:\n",
        "                eth_indices.append(i)\n",
        "\n",
        "        # Calculate accuracy for this ethnicity\n",
        "        eth_correct = sum(1 for i in eth_indices if y_pred_classes[i] == y_test[i])\n",
        "        eth_accuracy = eth_correct / len(eth_indices) if eth_indices else 0\n",
        "        ethnicity_accuracies[eth_name] = eth_accuracy\n",
        "\n",
        "    # Plot accuracies\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ethnicities = list(ethnicity_accuracies.keys())\n",
        "    accuracies = list(ethnicity_accuracies.values())\n",
        "\n",
        "    plt.bar(ethnicities, accuracies)\n",
        "    plt.title(f'Model Accuracy by Ethnicity - {base_name}\\nTrained on VGG16')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print accuracies\n",
        "    print(\"\\nAccuracies by Ethnicity:\")\n",
        "    for eth, acc in ethnicity_accuracies.items():\n",
        "        print(f\"{eth}: {acc:.4f}\")\n",
        "\n",
        "    # Add confusion matrix\n",
        "\n",
        "    # Convert y_test and y_pred_classes from person IDs to ethnicity labels\n",
        "    #eth_y_test = [id_to_ethnicity[label_to_id[label]] for label in y_test]\n",
        "    #eth_y_pred = [id_to_ethnicity[label_to_id[label]] for label in y_pred_classes]\n",
        "\n",
        "    #cm = confusion_matrix(y_test, y_pred_classes)\n",
        "    #plt.figure(figsize=(10,8))\n",
        "    #sns.heatmap(cm, annot=True, fmt='d')\n",
        "    #plt.title(f'Confusion Matrix - {base_name}')\n",
        "    #plt.ylabel('True Label')\n",
        "    #plt.xlabel('Predicted Label')\n",
        "    #plt.show()\n",
        "\n",
        "    return ethnicity_accuracies,  y_test, y_pred_classes"
      ],
      "metadata": {
        "id": "rSD29RpzbdUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the model"
      ],
      "metadata": {
        "id": "RVo9RPbdcOQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_outputs(model, id_to_label, history, training_filepath):\n",
        "   \"\"\"\n",
        "   Final function in the pipeline that saves the trained model and mappings\n",
        "   Input: model, label mapping, training history, and original training filepath\n",
        "   Output: saved model files\n",
        "   \"\"\"\n",
        "   # Get base name of training data to use in model name\n",
        "   base_name = os.path.splitext(os.path.basename(training_filepath))[0]\n",
        "\n",
        "   # Create model name that reflects training data used\n",
        "   model_name = f'face_recognition_VGG16_{base_name}.h5'\n",
        "   model_path = f'/content/drive/MyDrive/savedModels/{model_name}'\n",
        "\n",
        "   # Save the model\n",
        "   model.save(model_path)\n",
        "   print(f\"\\nModel saved as: {model_path}\")\n",
        "\n",
        "   # Save the id_to_label mapping (important for predictions!)\n",
        "   # Convert the id_to_label dictionary to strings (json requires string keys)\n",
        "   id_to_label_str = {str(k): int(v) for k, v in id_to_label.items()}\n",
        "   mapping_name = f'id_to_label_mapping_{base_name}.json'\n",
        "   mapping_path = f'/content/drive/MyDrive/savedModels/{mapping_name}'\n",
        "\n",
        "   with open(mapping_path, 'w') as f:\n",
        "        json.dump(id_to_label_str, f)\n",
        "\n",
        "\n",
        "   return model_name, mapping_name"
      ],
      "metadata": {
        "id": "MHW31jWAcMTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Final Pipeline function"
      ],
      "metadata": {
        "id": "2f0R0YfqBRse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(training_filepath):\n",
        "   # Setup all imports\n",
        "   setup_imports()\n",
        "   base_name = os.path.splitext(os.path.basename(training_filepath))[0]\n",
        "\n",
        "   if check_model_exists(base_name):\n",
        "       print(f\"Loading existing model for {base_name}\")\n",
        "       model = load_saved_model(base_name)\n",
        "       # Create and process training data (for evaluation only)\n",
        "       X, y, id_to_label, _ = create_training_data(training_filepath)\n",
        "       _, _, X_test, _, _, y_test = split_data(X, y)\n",
        "       ethnicity_accuracies = evaluate_model(model, X_test, y_test, id_to_label, base_name)\n",
        "       return f'face_recognition_VGG16_{base_name}.h5', f'id_to_label_mapping_{base_name}.json', ethnicity_accuracies\n",
        "\n",
        "   # Create and process training data\n",
        "   X, y, id_to_label, base_name = create_training_data(training_filepath)\n",
        "\n",
        "   # Split the data\n",
        "   X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
        "\n",
        "   # Create the model\n",
        "   num_classes = len(id_to_label)\n",
        "   model = create_model(num_classes)\n",
        "\n",
        "   # Train the model\n",
        "   model, history = train_model(model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "   # Evaluate the model\n",
        "   ethnicity_accuracies = evaluate_model(model, X_test, y_test, id_to_label, base_name)\n",
        "\n",
        "   # Save the outputs\n",
        "   model_name, mapping_name = save_outputs(model, id_to_label, history, training_filepath)\n",
        "\n",
        "   return model_name, mapping_name, ethnicity_accuracies"
      ],
      "metadata": {
        "id": "8Ta79UmcCbZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "weZubaubqmDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup all imports\n",
        "training_filepath = \"/content/drive/MyDrive/dataset/22w30a38b30i_50images_each.zip\"\n",
        "setup_imports()\n",
        "base_name = os.path.splitext(os.path.basename(training_filepath))[0]\n",
        "\n",
        "if check_model_exists(base_name):\n",
        "  print(f\"Loading existing model for {base_name}\")\n",
        "  model = load_saved_model(base_name)\n",
        "  # Create and process training data (for evaluation only)\n",
        "  X, y, id_to_label, _ = create_training_data(training_filepath)\n",
        "  _, _, X_test, _, _, y_test = split_data(X, y)\n",
        "  ethnicity_accuracies = evaluate_model(model, X_test, y_test, id_to_label, base_name)\n",
        "\n",
        "# Create and process training data\n",
        "X, y, id_to_label, base_name = create_training_data(training_filepath)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
        "\n",
        "# Create the model\n",
        "num_classes = len(id_to_label)\n",
        "model = create_model(num_classes)\n",
        "\n",
        "# Train the model\n",
        "model, history = train_model(model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Evaluate the model\n",
        "ethnicity_accuracies = evaluate_model(model, X_test, y_test, id_to_label, base_name)\n",
        "\n",
        "# Save the outputs\n",
        "model_name, mapping_name = save_outputs(model, id_to_label, history, training_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_Zz-cDJdt5O",
        "outputId": "b14893d1-9369-4187-fb7a-4fc0798a798e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully\n",
            "Dataset contents: ['African American', 'Caucasian Latin', 'Asian Indian', 'East Asian']\n",
            "Found 120 unique individuals\n",
            "Processing ethnicity folder: African American\n",
            "Processing ethnicity folder: Caucasian Latin\n",
            "Processing ethnicity folder: Asian Indian\n",
            "Processing ethnicity folder: East Asian\n",
            "Processing dataset from: 22w30a38b30i_50images_each\n",
            "Final dataset shapes - X: (6000, 224, 224, 3), y: (6000,)\n",
            "\n",
            "Split sizes:\n",
            "Training: 3600 images\n",
            "Validation: 1200 images\n",
            "Test: 1200 images\n",
            "Starting training...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion matrices"
      ],
      "metadata": {
        "id": "VwlARKxuKd0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create reverse mapping from label values to ethnicity values\n",
        "reverse_mapping = {}\n",
        "for key in id_to_label:\n",
        "    if key in id_to_ethnicity:\n",
        "        reverse_mapping[id_to_label[key]] = id_to_ethnicity[key]\n",
        "\n",
        "# Map predictions\n",
        "mapped_predictions = [reverse_mapping.get(int(pred), 'Unknown') for pred in y_pred_classes]\n",
        "mapped_true =        [reverse_mapping.get(int(it), 'Unknown') for it in y_test]\n",
        "\n",
        "print(f'PREDICTIONS: {len(mapped_predictions)}')\n",
        "print(f'TRUE: {len(mapped_true)}')"
      ],
      "metadata": {
        "id": "efmRC-CrnO6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(mapped_true, mapped_predictions)\n",
        "sns.heatmap(cm/np.sum(cm), annot=True, fmt='.1%', cmap=sns.color_palette(\"YlOrBr\", as_cmap=True),\n",
        "            xticklabels=['African American', 'Asian Indian', 'Caucasian Latin', 'East Asian'],\n",
        "            yticklabels=['African American', 'Asian Indian', 'Caucasian Latin', 'East Asian'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.xticks(rotation=30)\n",
        "plt.yticks(rotation=30)\n",
        "plt.title('Dominant Representation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7Q_oBpvoKgmy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "mjuJzhWmZQF0",
        "AueIN8FRaQqA"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}